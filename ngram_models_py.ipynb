{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAAc6nCg+aDo1y1i/24MfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pravallikabesi/NLP_LAB/blob/main/ngram_models_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX9NQB9RZ0kr",
        "outputId": "50fe3035-cd47-4c7d-e24e-8f28ef69897d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 0.125, 'love': 0.125, 'natural': 0.0625, 'language': 0.1875, 'processing': 0.0625, 'models': 0.125, 'are': 0.0625, 'fun': 0.0625, 'to': 0.0625, 'build': 0.0625, 'building': 0.0625}\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def unigram_model(corpus):\n",
        "    words = \" \".join(corpus).lower().split()\n",
        "    counts = Counter(words)\n",
        "    total = sum(counts.values())\n",
        "    return {w: c / total for w, c in counts.items()}\n",
        "\n",
        "# Example\n",
        "corpus = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Language models are fun to build\",\n",
        "    \"I love building language models\"\n",
        "]\n",
        "\n",
        "print(unigram_model(corpus))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def simple_bigram_model(corpus):\n",
        "    words = \" \".join(corpus).lower().split()\n",
        "    bigrams = [(words[i], words[i+1]) for i in range(len(words) - 1)]\n",
        "    bigram_counts = Counter(bigrams)\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    model = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 not in model:\n",
        "            model[w1] = {}\n",
        "        model[w1][w2] = count / word_counts[w1]\n",
        "    return model\n",
        "\n",
        "# Example\n",
        "corpus = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Language models are fun to build\",\n",
        "    \"I love building language models\"\n",
        "]\n",
        "\n",
        "print(simple_bigram_model(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcPIBG5dbuH3",
        "outputId": "de4761de-c74c-40c3-e525-deb1a4c042e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': {'love': 1.0}, 'love': {'natural': 0.5, 'building': 0.5}, 'natural': {'language': 1.0}, 'language': {'processing': 0.3333333333333333, 'models': 0.6666666666666666}, 'processing': {'language': 1.0}, 'models': {'are': 0.5}, 'are': {'fun': 1.0}, 'fun': {'to': 1.0}, 'to': {'build': 1.0}, 'build': {'i': 1.0}, 'building': {'language': 1.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89cde90f",
        "outputId": "41a2f514-70cb-4e07-e6aa-2fc189851376"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def trigram_model(corpus):\n",
        "    words = \" \".join(corpus).lower().split()\n",
        "    trigrams = [(words[i], words[i+1], words[i+2]) for i in range(len(words) - 2)]\n",
        "    counts = defaultdict(lambda: defaultdict(int))\n",
        "    for w1, w2, w3 in trigrams:\n",
        "        counts[(w1, w2)][w3] += 1\n",
        "\n",
        "    model = defaultdict(lambda: defaultdict(float))\n",
        "    for (w1, w2) in counts:\n",
        "        total_w1_w2 = sum(counts[(w1, w2)].values())\n",
        "        for w3 in counts[(w1, w2)]:\n",
        "            model[(w1, w2)][w3] = counts[(w1, w2)][w3] / total_w1_w2\n",
        "    return model\n",
        "\n",
        "# Example\n",
        "corpus = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Language models are fun to build\",\n",
        "    \"I love building language models\"\n",
        "]\n",
        "\n",
        "print(trigram_model(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<function trigram_model.<locals>.<lambda> at 0x7d4a64482a20>, {('i', 'love'): defaultdict(<class 'float'>, {'natural': 0.5, 'building': 0.5}), ('love', 'natural'): defaultdict(<class 'float'>, {'language': 1.0}), ('natural', 'language'): defaultdict(<class 'float'>, {'processing': 1.0}), ('language', 'processing'): defaultdict(<class 'float'>, {'language': 1.0}), ('processing', 'language'): defaultdict(<class 'float'>, {'models': 1.0}), ('language', 'models'): defaultdict(<class 'float'>, {'are': 1.0}), ('models', 'are'): defaultdict(<class 'float'>, {'fun': 1.0}), ('are', 'fun'): defaultdict(<class 'float'>, {'to': 1.0}), ('fun', 'to'): defaultdict(<class 'float'>, {'build': 1.0}), ('to', 'build'): defaultdict(<class 'float'>, {'i': 1.0}), ('build', 'i'): defaultdict(<class 'float'>, {'love': 1.0}), ('love', 'building'): defaultdict(<class 'float'>, {'language': 1.0}), ('building', 'language'): defaultdict(<class 'float'>, {'models': 1.0})})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Now you can use word_tokenize and sent_tokenize without errors\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello! I'm learning Natural Language Processing. It's fun.\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\", sentences)\n",
        "\n",
        "# Word tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"Words:\", words)\n"
      ],
      "metadata": {
        "id": "BuEbekhwfZT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687da277-b3e8-4718-f546-e2f6fb351e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Hello!', \"I'm learning Natural Language Processing.\", \"It's fun.\"]\n",
            "Words: ['Hello', '!', 'I', \"'m\", 'learning', 'Natural', 'Language', 'Processing', '.', 'It', \"'s\", 'fun', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIWzdHeGfY4G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}